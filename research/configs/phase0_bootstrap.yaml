# Phase 0 Bootstrap Training Configuration
# Target: 30-40% win rate vs easy AI
# Training: 100K-500K steps with comprehensive TensorBoard logging

experiment:
  name: "phase0_bootstrap"
  phase: 0
  target_win_rate: 0.35 # 35% target win rate
  description: "Bootstrap training experiment with comprehensive TensorBoard logging"

training:
  total_timesteps: 100000 # Initial, extend to 500000 if needed
  eval_freq: 10000
  checkpoint_freq: 10000
  save_best_model: true
  tensorboard_log: true
  tensorboard_verbose: true
  log_interval: 1000
  verbose: 2

  # Training continuation settings
  continue_training: false # Set to true to continue from checkpoint
  checkpoint_path: null # Path to checkpoint to continue from
  reset_num_timesteps: false # Continue timestep counter from checkpoint

  # Early stopping parameters
  early_stopping: true
  early_stopping_patience: 100000 # 100K steps
  early_stopping_min_reward: 0.3 # Stop if we achieve 30% win rate

  # Checkpoint parameters
  checkpoint_dir: "./artifacts/checkpoints/phase0_bootstrap"
  tensorboard_log_dir: "./logs/phase0_bootstrap"

  # Performance monitoring
  performance_benchmarking: true
  log_system_metrics: true

environment:
  name: BootstrapClashRoyaleEnv
  action_space: MultiDiscrete([5, 32, 18]) # Including "no action"
  observation_space: Box(low=-1, high=1, shape=(53,))

  # Screen capture settings
  window_name: "BlueStacks App Player 1"
  resolution: "1920x1080"
  roi: null # Full screen

  # Performance settings
  max_step_time_ms: 5000.0 # Increased to prevent warnings
  action_delay_ms: 1500.0 # Reduced for faster training

  # Game settings
  card_names:
    [
      "archers",
      "giant",
      "knight",
      "mini_pekka",
      "goblin_hut",
      "minions",
      "musketeer",
      "valkyrie",
    ]

  # Manual outcome input for Phase 0 (when automatic detection is not available)
  manual_outcome_input: false
  outcome_check_delay_seconds: 30 # Check for outcome after 40 seconds

policy:
  name: StructuredMLPPolicy
  architecture: "BootstrapActorCriticPolicy"
  features_extractor_kwargs:
    features_dim: 53

  # Policy configuration
  action_space: [5, 32, 18] # 5 card options (4 cards + no action), 32x18 grid
  state_dim: 53
  device: "cpu" # Force CPU for stability

ppo:
  # PPO hyperparameters (Phase 0 uses SB3 defaults)
  learning_rate: 3e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.05 # Increased from 0.01 to encourage more exploration
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: false
  sde_sample_freq: -1
  target_kl: 0.01

  # Training stability
  normalize_advantages: true
  normalize_rewards: false

logging:
  # Comprehensive logging settings
  log_interval: 1000
  verbose: 2
  tensorboard_log: "all"

  # Enhanced logging features
  log_action_distributions: true
  log_elixir_efficiency: true
  log_tower_damage: true
  log_reward_components: true
  log_performance_metrics: true

  # Visual logging
  log_action_heatmaps: true
  log_card_selection_frequency: true
  log_state_distributions: true
  log_model_weights: true

  # Logging frequency
  heatmap_log_freq: 5000
  distribution_log_freq: 2000
  model_log_freq: 10000

evaluation:
  # Evaluation settings
  n_eval_episodes: 10
  eval_freq: 10000
  deterministic: true

  # Success criteria
  target_win_rate: 0.35
  min_episodes: 20

  # Evaluation logging
  log_eval_actions: true
  log_eval_states: true
  save_eval_videos: false

outputs:
  # Output directories
  checkpoint_dir: "./artifacts/checkpoints/phase0_bootstrap"
  log_dir: "./logs/phase0_bootstrap"
  output_dir: "./research/outputs/phase0_bootstrap"

  # Outputs to generate
  save_learning_curves: true
  save_action_heatmaps: true
  save_performance_plots: true
  save_training_summary: true

system:
  # System requirements
  device: "cpu"
  num_threads: 4
  memory_limit_gb: 8

  # Reproducibility
  seed: 42
  deterministic_torch: true

  # Performance monitoring
  monitor_gpu_usage: false
  monitor_cpu_usage: true
  log_memory_usage: true
